This is an instruction of how to generate test instances and data format of
these instances.

All data are generated by R scripts.

- generateAll.R
	- generateTestCases.R

- generateAll.R calls all sub-functions to generate test instances.
				It creates all directories of test instances.

=================================================

We simulated HP ProLiant DL380 G7 servers with different number of cpus and
memories.



--------- PMConfig_small.csv ---------

CPU capacity: 3.3 GHz * 4 cores = 13200 MHz
Memory capacity: 16 GB = 16000 MB
Energy: 540 W

--------- PMConfig_medium.csv ---------

CPU capacity: 3.3 GHz * 8 cores = 26400 MHz
Memory capacity: 32 GB = 32000 MB
Energy: 1080 W

--------- PMConfig_large.csv ---------

CPU capacity: 3.3 GHz * 16 cores = 52800 MHz
Memory capacity: 64 GB = 64000 MB
Energy: 2160 W

--------- PMConfig_xLarge.csv ---------

CPU capacity: 3.3 GHz * 36 cores = 118800 MHz
Memory capacity: 72  GB = 72000 MB
Energy: 19440 W



=================================================

We simulated 5 types of VMs according to real world VM configuration from AWS
in
--------- VMConfig_small.csv ---------

VM types	CPU    	Mem
t2.nano: 	3300 MHz	500 MB
t2.micro 	3300 MHz	1000 MB
t2.small	6600 MHz 	2000 MB
t2.medium	6600 MHz 	4000 MB
t2.xlarge	13200 MHz	16000 MB


We simulated 5 types of VMs according to real world VM configuration from AWS
in
--------- VMConfig_medium.csv ---------

VM types	CPU    	Mem
t2.nano: 	3300 MHz	2000 MB
t2.medium 	6600 MHz	4000 MB
t2.large	6600 MHz 	8000 MB
t2.xlarge	13200 MHz	16000 MB
t2.2xlarge	26400 MHz	32000 MB



We simulated 5 types of VMs according to real world VM configuration from AWS
in
--------- VMConfig_large.csv ---------
VM types	CPU    	Mem
t2.medium 	6600 MHz	4000 MB
t2.large	6600 MHz 	8000 MB
t2.xlarge	13200 MHz	16000 MB
t2.2xlarge	26400 MHz	32000 MB
m5.4xlarge	52800 MHz	64000 MB


We simulated 5 types of VMs according to real world VM configuration from AWS
in
--------- VMConfig_xLarge.csv ---------
VM types	CPU    	Mem
t2.large	6600 MHz 	8000 MB
t2.xlarge	13200 MHz	16000 MB
t2.2xlarge	26400 MHz	32000 MB
c5.4xlarge	52800 MHz	32000 MB
c5.9xlarge	118800 MHz	72000 MB


=================================================
We obtain the real world dataset from
http://gwa.ewi.tudelft.nl/datasets/gwa-t-12-bitbrains. 

We use the GWA-T-12 fastStorage trace which contains the real world
applications. These applications are typically used for financial reporting. 

These datasets contains large amount of trace. We only use a small protion. 
We generate our source dataset from the first 100 trace files (1000+) from the origin dataset.
We only select the CPU usage (in terms of MHz) and Memoery (originally in
terms of KB). 
We first eliminate the record with 0.0 in CPU and memory usage. 
Then, we convert the memory in terms of MB. 

Our source dataset contains more than four million records. 
We randomly select records from the source dataset to be our test cases. 

=================================================

- generateTestCases.R generates container requirements files 

It can generate artificial data or real world data.

generateTask(whichDataSet, whichVMsize, size, testCase) has four parameters

whichDataSet: 	1 means real world data, 
				2 means artificial data 

whichVMsize: 	1 means VMConfig_small,
				2 means VMConfig_medium,
				3 means VMConfig_large,
				4 means VMConfig_xLarge,


-------------------- artificial data ----------------------------

We generate requirements of CPU with real world dataset WS-Dream combined
with synthetic data from exponential distribution. 
	
For each test case, we first randomly select a throughput from WS-Dream
application throughput dataset. Then, we randomly generate a CPU requirements
for a single task of the application. Then, the CPU requirement is calculated
by multiply CPU requirement and throughput. The same rules apply for
taskMem.csv. 

We define the coefficient in exponential distribution as 0.001.

-------------------- real world data ----------------------------

We generate real world data from a real-world dataset mentioned above.

we randomly select a certain number of test cases from 4 million of records. 


=================================================
generateOScases

- generateOScases generates container OS requirements files 
	Each number represents a type of OS for a container.

We design three big categories: single OS type, three OS types, five OS types.

We generate OS requirements according to a survey of OS in cloud data
center.

-------------------- Three OS cases ----------------------------

We choose the top three OS types. Their market percentage is roughly 0.5, 0.3 and
0.2.

-------------------- Five OS cases ----------------------------
We choose the OS distribution according to this survey. 
https://community.spiceworks.com/networking/articles/2462-server-virtualization-and-os-trends

OS1's percentage = 17.9%
OS2's percentage = 45.4%
OS3's percentage = 23.6%
OS4's percentage = 10.5%
OS5's percentage = 2.6%


=================================================

Experiment Design

Currently, we design 200 datasets with increasing number of applications to be
deployed.

Test instances (There are 50 test cases on each size):

number of applications: 20, 80, 200, 5000

